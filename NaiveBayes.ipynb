{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An implementation of Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.X_train, self.y_train = [], []\n",
    "\n",
    "        self.classes = np.array(['A', 'B', 'E', 'V'])\n",
    "        \n",
    "        self.unique_words = {}  # key: unique_word in the entire file. value: frequency of that word\n",
    "        self.frequent_words = []\n",
    "        \n",
    "        self.priors = None\n",
    "        self.likelihoods = None\n",
    "        \n",
    "        \n",
    "    def read(self, training_file, testing_file):\n",
    "        \"\"\"\n",
    "        Read all the data from 3 files. Process the file into datasets and set the corresponding attribute.\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(training_file, 'r') as train, open(testing_file, 'r') as test:\n",
    "            \n",
    "            # Skip first line\n",
    "            train_lines = train.readlines()[1:]  \n",
    "            test_lines = test.readlines()[1:]\n",
    "        \n",
    "        \"\"\"###########################  Training Data  ###########################\"\"\"\n",
    "        \n",
    "        n_samples = len(train_lines)\n",
    "        n_words = 2500  # how many feature the modle will use\n",
    "        \n",
    "        y_train = []\n",
    "        X_train = []\n",
    "        \n",
    "        self.vocabulary = [] # Vocabulary of every abstract from the file. With each element being a dict of words from each abstract\n",
    "\n",
    "        for line in train_lines:\n",
    "            \n",
    "            segment = line.rstrip().split(',')\n",
    "            y = segment[1]\n",
    "            words = segment[2].replace('\\\"', '').split(' ') # Remove the quotation mark \"\n",
    "            \n",
    "            # Remove any unnecessary word\n",
    "            new_words = []\n",
    "            vocabulary = {}  # Unique word in this abstract with freq\n",
    "            \n",
    "            for word in words:\n",
    "                \n",
    "                \"\"\"\n",
    "                Preprocess the data to remove unnecessary words.\n",
    "                \"\"\" \n",
    "                \n",
    "                if len(word) <= 4:\n",
    "                    continue\n",
    "                if len(word) >= 9:\n",
    "                    continue\n",
    "                if word.isdigit() or any(char.isdigit() for char in word):\n",
    "                    continue\n",
    "                if any(word in existing_word for existing_word in new_words):  # Check if word is substring of any existing word\n",
    "                    continue\n",
    "                \n",
    "                new_words.append(word)\n",
    "                vocabulary[word] = vocabulary.get(word, 0) + 1  # Add one count to current vocabulary\n",
    "                self.unique_words[word] = self.unique_words.get(word, 0) + 1  # Unique word in the entire file\n",
    "\n",
    "            X = np.array(new_words)\n",
    "            self.vocabulary.append(vocabulary)\n",
    "            \n",
    "            y_train.append(y)\n",
    "            X_train.append(X)\n",
    "        \n",
    "        self.y_train = np.array(y_train)\n",
    "        \n",
    "        # Identify the 1000 most frequently occurring words after preprocessing and generate 0-1  \n",
    "        # attributes stating whether or not the word occurs in the corresponding abstract.\n",
    "        self.frequent_words = [x[0] for x in sorted(self.unique_words.items(), key=lambda x: x[1], reverse=True)][:n_words]\n",
    "\n",
    "        # Init to be a n_samples by n_words zero matrix\n",
    "        self.X_train = np.zeros((n_samples, n_words))\n",
    "        \n",
    "        # Check whether a frequent word appear in the corresponding \n",
    "        # abstract, set the cooresponding attribute to its frequency\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_words):\n",
    "                if self.frequent_words[j] in X_train[i]:\n",
    "                    self.X_train[i][j] = self.vocabulary[i][self.frequent_words[j]]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"###########################  Testing Data  ###########################\"\"\"\n",
    "\n",
    "        X_test = []\n",
    "        \n",
    "        # Process the lines from the input testing file\n",
    "        for line in test_lines:\n",
    "            \n",
    "            words = line.rstrip().split(',')[1].replace('\\\"', '').split(' ')\n",
    "            \n",
    "            # Init X to be a zero vector of n_words attributes\n",
    "            X = np.zeros(n_words)\n",
    "            \n",
    "            for word in words:\n",
    "                \n",
    "                if len(word) <= 4:\n",
    "                    continue\n",
    "                if len(word) >= 9:\n",
    "                    continue\n",
    "                if word.isdigit() or any(char.isdigit() for char in word):\n",
    "                    continue\n",
    "                \n",
    "                # if a word is in the frequent word list, increment its attribute value\n",
    "                for j in range(n_words):\n",
    "                    if word == self.frequent_words[j]:\n",
    "                        X[j] += 1\n",
    "            \n",
    "            # Done processing one line\n",
    "            X_test.append(X)\n",
    "        \n",
    "        # Entire testing data\n",
    "        self.X_test = np.array(X_test)\n",
    "        \n",
    "\n",
    "    def train(self, X=None, y=None):\n",
    "        \"\"\"Arguments:\n",
    "            X: X_training type: 2d np.array\n",
    "            y: y_training type: 1d np.array\n",
    "           Generate priors and likelihoods from the training examples.\n",
    "        \"\"\"\n",
    "        # Set default values\n",
    "        if X is None: X = self.X_train\n",
    "        if y is None: y = self.y_train\n",
    "        \n",
    "        n_samples, n_words = X.shape\n",
    "        n_classes = 4\n",
    "        \n",
    "        # Init priors and likelihoods\n",
    "        self.priors = np.zeros(n_classes, dtype=np.float64)\n",
    "        self.likelihoods = np.zeros((n_classes, n_words), dtype=np.float64)\n",
    "        \n",
    "        for i, c in enumerate(self.classes):\n",
    "\n",
    "            X_c = X[c==y]  # Select all instances X with class being c\n",
    "            \n",
    "            self.priors[i] = X_c.shape[0] / float(n_samples)\n",
    "            self.likelihoods[i] = (X_c.sum(axis=0) + 1) / (float(X_c.sum() + n_words))  # 1000 being total number of attributes   \n",
    "\n",
    "     \n",
    "    def _predict(self, x):\n",
    "        \"\"\"\n",
    "        Run the classifer on one single instance x.\n",
    "        Return the prediction class.\n",
    "        \"\"\"\n",
    "        posteriors = np.zeros(4)  # Empty posterior probability for four differen classes\n",
    "        \n",
    "        for i, c in enumerate(self.classes):\n",
    "            \n",
    "            # Doing calculation in log space\n",
    "            prior = np.log(self.priors[i])\n",
    "            likelihood = np.dot(x, np.log(self.likelihoods[i]))\n",
    "#             likelihood = np.sum(np.log(self.likelihoods[i]))\n",
    "            posteriors[i] = prior + likelihood\n",
    "        \n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "    \n",
    "    \n",
    "    def test(self, filename_to_write):\n",
    "        \"\"\"\n",
    "        A wrapper function to run the method _predict() on every single \n",
    "        instance from the testing data and write result to file_to_write.\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(filename_to_write, 'w') as f:\n",
    "            \n",
    "            f.write(f\"id,class\\n\")  # First line \n",
    "\n",
    "            for i, X in enumerate(self.X_test):\n",
    "                \n",
    "                predict = self._predict(X)\n",
    "\n",
    "                f.write(f\"{i+1},{predict}\\n\")  # str to write\n",
    "            \n",
    "\n",
    "    \n",
    "    def k_fold_cross_validation(self, k_folds):\n",
    "        \"\"\"\n",
    "        Split the data into k equal sized folds, select one fold as test set and use the rest as \n",
    "        training set. Contiunue the process for k times until every fold has been selected as\n",
    "        a test set once. Return the accuracy.\n",
    "        \n",
    "        Assuming k_folds is at least 2.\n",
    "        \"\"\"\n",
    "        \n",
    "        fold_size = int(len(self.X_train) / k_folds)\n",
    "        total_count = fold_size * k_folds\n",
    "        correct_count = 0\n",
    "\n",
    "        # Run the classifier k_folds times on k_folds different test sets.\n",
    "        for i in range(k_folds):\n",
    "\n",
    "            # Split dataset into testing and training\n",
    "            X_test = self.X_train[i*fold_size : (i+1)*fold_size]\n",
    "            y_test = self.y_train[i*fold_size : (i+1)*fold_size]\n",
    "\n",
    "            if i == 0:\n",
    "                X_train = self.X_train[(i+1)*fold_size:]\n",
    "                y_train = self.y_train[(i+1)*fold_size:]\n",
    "            elif i == k_folds - 1:\n",
    "                X_train = self.X_train[:i*fold_size]\n",
    "                y_train = self.y_train[:i*fold_size]\n",
    "            else:\n",
    "                X_train = np.concatenate((self.X_train[:i*fold_size], self.X_train[(i+1)*fold_size:]))\n",
    "                y_train = np.concatenate((self.y_train[:i*fold_size], self.y_train[(i+1)*fold_size:]))\n",
    "\n",
    "            # Train the model on the selected training set\n",
    "            self.train(X_train, y_train)\n",
    "\n",
    "            # Run the learned model on the selected test set\n",
    "            for j, x in enumerate(X_test):\n",
    "                predict = self._predict(x)  # Predicted class of test data\n",
    "                target = y_test[j]  # Target class of test data\n",
    "\n",
    "                if predict == target:\n",
    "                    correct_count += 1\n",
    "        \n",
    "        # Return the accuracy on the test set\n",
    "        return correct_count / total_count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&#39; Finish loading the data from file &#39;"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "\"\"\" Load the data\"\"\"\n",
    "\"\"\" Please expect a runnint time of about 30s\"\"\"\n",
    "\n",
    "# Please expect about 30s running time to load data\n",
    "model = NaiveBayes()\n",
    "model.read(\"trg.csv\", \"tst.csv\")\n",
    "\n",
    "\"\"\" Finish loading the data from file \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy on 10-fold cross validation: 91.45%\n"
    }
   ],
   "source": [
    "# Train the model on the training set from input file\n",
    "model.train()\n",
    "\n",
    "# Run the model on the testing set from input file\n",
    "# And write result to \"solution.csv\"\n",
    "model.test(\"rhua966.csv\")\n",
    "\n",
    "# Run 5-fold cross validation on the training set from input file and get the accuracy of the model\n",
    "k = 10\n",
    "accuracy = model.k_fold_cross_validation(k)\n",
    "print(f\"Accuracy on {k}-fold cross validation: {100 * accuracy:5.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}